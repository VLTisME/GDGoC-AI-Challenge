{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f2ca5a",
   "metadata": {},
   "source": [
    "# Import libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09695619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "# use gpu if available else cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89583a",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IM_DIR  = \"/kaggle/input/gd-go-c-hcmus-aic-fragment-segmentation-track/train/images\"\n",
    "MSK_DIR = \"/kaggle/input/gd-go-c-hcmus-aic-fragment-segmentation-track/train/masks\"\n",
    "\n",
    "all_fnames = sorted(os.listdir(IM_DIR))\n",
    "train_fnames, val_fnames = train_test_split(all_fnames, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd1bab",
   "metadata": {},
   "source": [
    "## Convert to detectron2's standard input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "def register_lazy(split, fnames):\n",
    "    name = f\"fragments_{split}\"\n",
    "    DatasetCatalog.register(\n",
    "        name,\n",
    "        lambda fnames=fnames: [\n",
    "            {\n",
    "                \"file_name\": os.path.join(IM_DIR, fn),\n",
    "                \"mask_file\": os.path.join(MSK_DIR, os.path.splitext(fn)[0] + \".png\"),\n",
    "                \"image_id\": idx,\n",
    "            }\n",
    "            for idx, fn in enumerate(fnames)\n",
    "        ]\n",
    "    )\n",
    "    MetadataCatalog.get(name).set(thing_classes=[\"fragment\"])\n",
    "\n",
    "register_lazy(\"train\", train_fnames)\n",
    "register_lazy(\"val\",   val_fnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from detectron2.data import detection_utils as utils, transforms as T\n",
    "from detectron2.structures import BoxMode, Instances\n",
    "\n",
    "def lazy_mapper(dataset_dict):\n",
    "    dataset_dict = dataset_dict.copy()\n",
    "    # load img\n",
    "    img = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    mask = np.array(Image.open(dataset_dict[\"mask_file\"]).convert(\"RGB\"))\n",
    "    pixels = mask.reshape(-1, 3)\n",
    "    unique_colors = np.unique(pixels, axis=0)\n",
    "    instance_colors = [tuple(c) for c in unique_colors if tuple(c) != (0, 0, 0)]\n",
    "\n",
    "    annos = []\n",
    "    for color in instance_colors:\n",
    "        binary_mask = (mask == color).all(axis=2).astype(np.uint8)\n",
    "        ys, xs = np.where(binary_mask)\n",
    "        if ys.size == 0:\n",
    "            continue\n",
    "        xmin, xmax = xs.min(), xs.max()\n",
    "        ymin, ymax = ys.min(), ys.max()\n",
    "        annos.append({\n",
    "            \"bbox\": [xmin, ymin, xmax, ymax],\n",
    "            \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "            \"segmentation\": binary_mask,\n",
    "            \"category_id\": 0\n",
    "        })\n",
    "    \n",
    "    # simple augmentation\n",
    "    aug_list = [\n",
    "        T.ResizeShortestEdge(short_edge_length=(400, 600), max_size=600),\n",
    "        T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "    ]\n",
    "\n",
    "    aug_input = T.AugInput(img)\n",
    "    transforms = T.AugmentationList(aug_list)(aug_input)\n",
    "    img = aug_input.image\n",
    "\n",
    "    # Transform annotations accordingly\n",
    "    annos_transformed = []\n",
    "    for obj in annos:\n",
    "        obj = obj.copy()\n",
    "        # transform_instance_annotations does not support bitmask segmentation\n",
    "        # so we remove it, transform bbox etc., and later re-attach it\n",
    "        mask = obj.pop(\"segmentation\")\n",
    "        obj = utils.transform_instance_annotations(obj, transforms, img.shape[:2])\n",
    "        # apply same transform to the mask\n",
    "        mask_image = Image.fromarray(mask * 255)\n",
    "        mask_transformed = transforms.apply_segmentation(np.array(mask_image)) // 255\n",
    "        obj[\"segmentation\"] = mask_transformed.astype(np.uint8)\n",
    "        annos_transformed.append(obj)\n",
    "    \n",
    "    annos = annos_transformed\n",
    "\n",
    "    # prepare instances\n",
    "    instances = utils.annotations_to_instances(\n",
    "        annos,\n",
    "        img.shape[:2],\n",
    "        mask_format=\"bitmask\"\n",
    "    )\n",
    "\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(img.transpose(2, 0, 1).copy())\n",
    "    dataset_dict[\"instances\"] = instances\n",
    "    dataset_dict[\"height\"] = img.shape[0]\n",
    "    dataset_dict[\"width\"] = img.shape[1]\n",
    "\n",
    "    return dataset_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b6087",
   "metadata": {},
   "source": [
    "# Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "import os\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "    \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "))\n",
    "\n",
    "cfg.DATASETS.TRAIN            = (\"fragments_train\",)\n",
    "cfg.DATASETS.TEST             = (\"fragments_val\",)\n",
    "cfg.DATALOADER.NUM_WORKERS    = 4\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES       = 1\n",
    "cfg.MODEL.BACKBONE.FREEZE_AT          = 2   # freeze first ResNet block\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "\n",
    "cfg.INPUT.MIN_SIZE_TRAIN      = (400,)\n",
    "cfg.INPUT.MIN_SIZE_TEST       = 400\n",
    "\n",
    "cfg.OUTPUT_DIR                = \"./detectron2_output\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH      = 8\n",
    "cfg.SOLVER.BASE_LR            = 0.002\n",
    "cfg.SOLVER.MAX_ITER           = 5000\n",
    "cfg.SOLVER.STEPS              = []       # no LR decay\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD  = 500\n",
    "cfg.SOLVER.LOGGING_PERIOD     = 10\n",
    "\n",
    "# Mixed precision\n",
    "cfg.SOLVER.AMP.ENABLED        = True\n",
    "\n",
    "# RPN proposals\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN  = 1200\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = 1200\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TEST   = 700\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST  = 700\n",
    "\n",
    "# turn off val‐set eval during training\n",
    "cfg.TEST.EVAL_PERIOD         = 0\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE        = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ecb6a",
   "metadata": {},
   "source": [
    "## Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452008e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine.hooks import HookBase\n",
    "\n",
    "class BestCheckpointer(HookBase):\n",
    "    def __init__(self, eval_period, trainer):\n",
    "        self.eval_period = eval_period\n",
    "        self.trainer = trainer\n",
    "        self.best_metric = -1\n",
    "\n",
    "    def after_step(self):\n",
    "        if (self.trainer.iter + 1) % self.eval_period == 0:\n",
    "            metrics = self.trainer.storage.latest()\n",
    "            iou = metrics.get(\"segm/AP\", 0)\n",
    "            if iou > self.best_metric:\n",
    "                self.best_metric = iou\n",
    "                self.trainer.checkpointer.save(f\"model_best_{iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32774a87",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27713f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=lazy_mapper)\n",
    "\n",
    "# Then later, instead of DefaultTrainer(cfg):\n",
    "trainer = MyTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "# with that set up, estimated training time on Kaggle, with GPU T4 x2, is 5 hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f1538",
   "metadata": {},
   "source": [
    "## Save model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b250aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./detectron2_output/mask_rcnn_R_50_FPN_3x.yaml\", \"w\") as f:\n",
    "    f.write(cfg.dump())\n",
    "print(\"Wrote:\", \"./detectron2_output/mask_rcnn_R_50_FPN_3x.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130691bb",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a40531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "# initialize model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.WEIGHTS = \"./detectron2_output/model_final.pth\"\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# predict\n",
    "img_path = \"/kaggle/input/gd-go-c-hcmus-aic-fragment-segmentation-track/train/images/002.jpg\"\n",
    "msk_path = \"/kaggle/input/gd-go-c-hcmus-aic-fragment-segmentation-track/train/masks/002.png\"\n",
    "imge = Image.open(img_path).convert('RGB')\n",
    "mske = Image.open(msk_path).convert('RGB')\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))   # 1 row, 2 columns\n",
    "axs[0].imshow(np.array(imge))\n",
    "axs[0].set_title(\"Image\")\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(np.array(mske))\n",
    "axs[1].set_title(\"Mask\")\n",
    "axs[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "img_bgr = cv2.imread(img_path)\n",
    "outputs = predictor(img_bgr)\n",
    "instances = outputs[\"instances\"].to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "masks = instances.pred_masks.cpu().numpy()  # (N, H, W)\n",
    "H, W = masks.shape[1:]\n",
    "\n",
    "N = masks.shape[0]\n",
    "cmap = plt.get_cmap('tab20')\n",
    "colors = (np.array([cmap(i / N)[:3] for i in range(N)]) * 255).astype(np.uint8)  # shape (N, 3)\n",
    "mosaic = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "for i, mask in enumerate(masks):\n",
    "    mosaic[mask] = colors[i % len(colors)]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(mosaic)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc20519",
   "metadata": {},
   "source": [
    "# CDF Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def draw_cdf(masks, pixel_size_mm=3):\n",
    "    # 1) compute diameters in cm\n",
    "    pixel_size_cm = pixel_size_mm / 10.0\n",
    "    areas_px      = masks.sum(axis=(1,2))             # pixel² per fragment\n",
    "    areas_cm2     = areas_px * (pixel_size_cm**2)     # cm²\n",
    "    diam_cm       = np.sqrt(areas_cm2)                # cm\n",
    "    N             = len(diam_cm)\n",
    "\n",
    "    # 2) sort & empirical CDF values\n",
    "    d_sorted = np.sort(diam_cm)\n",
    "    y_full   = np.arange(1, N+1) / N * 100            # 1/N, 2/N, …, 1.0 → in %\n",
    "\n",
    "    # 2b) prepend the (0, 0) anchor so CDF starts at zero\n",
    "    d_plot = np.concatenate([[0.0], d_sorted])\n",
    "    y_plot = np.concatenate([[0.0], y_full])\n",
    "\n",
    "    # 3) key statistics\n",
    "    Dmin, Dmax, Dmean = d_sorted[0], d_sorted[-1], d_sorted.mean()\n",
    "    D10, D50, D90     = np.percentile(d_sorted, [10, 50, 90])\n",
    "\n",
    "    # 4) start plotting\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.plot(d_plot, y_plot, '-o', color='blue', label='CDF')\n",
    "    # If you prefer a step‐plot, comment the line above and uncomment below:\n",
    "    # ax.step(d_plot, y_plot, where='post', color='blue', label='CDF')\n",
    "\n",
    "    # 5) horizontal guides @10,50,90%\n",
    "    for pct, col in zip([10,50,90], ['cyan','magenta','blue']):\n",
    "        ax.axhline(pct, color=col, linestyle=':', lw=1)\n",
    "        ax.text(100, pct, f'{pct}%', color=col,\n",
    "                va=('bottom' if pct==10 else 'top' if pct==90 else 'center'),\n",
    "                ha='right')\n",
    "\n",
    "    # 6) vertical percentile lines @D10,D50,D90\n",
    "    for dval, pct, col in zip([D10,D50,D90], [10,50,90], ['cyan','magenta','blue']):\n",
    "        ax.axvline(dval, color=col, linestyle=':', lw=2)\n",
    "        ax.text(dval, pct, f'D{pct}: {dval:.2f}',\n",
    "                color=col, fontsize=9,\n",
    "                va=('bottom' if pct==10 else 'top' if pct==90 else 'center'),\n",
    "                ha='left', backgroundcolor='white')\n",
    "\n",
    "    # 7) vertical lines @Dmin, mean, Dmax\n",
    "    ax.axvline(Dmin,  color='green',  linestyle='--', lw=2)\n",
    "    ax.axvline(Dmean, color='orange', linestyle='--', lw=2)\n",
    "    ax.axvline(Dmax,  color='red',    linestyle='--', lw=2)\n",
    "\n",
    "    ax.text(Dmin,  0,   f'Dmin: {Dmin:.2f}',  color='green',\n",
    "            va='bottom', ha='left', fontsize=9)\n",
    "    ax.text(Dmean, 50,  f'Average: {Dmean:.2f}', color='orange',\n",
    "            va='center', ha='left', fontsize=9)\n",
    "    ax.text(Dmax,  100, f'Dmax: {Dmax:.2f}',  color='red',\n",
    "            va='top',    ha='right', fontsize=9)\n",
    "\n",
    "    # 8) axes formatting\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_xticks([0,20,40,60,80,100])\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.set_xlabel('Fragment Size (cm)')\n",
    "    ax.set_ylabel('Cumulative Percentage (%)')\n",
    "    ax.set_title(\n",
    "        'Cumulative Distribution Function (CDF) of Fragment Sizes\\n'\n",
    "        f'Total Fragments: {N}'\n",
    "    )\n",
    "    ax.grid(linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # 9) custom legend\n",
    "    handles = [\n",
    "        Line2D([0],[0], color='blue',   marker='o', linestyle='-',  label='CDF'),\n",
    "        Line2D([0],[0], color='green',  linestyle='--', lw=2,       label=f'Dmin: {Dmin:.2f}'),\n",
    "        Line2D([0],[0], color='orange', linestyle='--', lw=2,       label=f'Average: {Dmean:.2f}'),\n",
    "        Line2D([0],[0], color='red',    linestyle='--', lw=2,       label=f'Dmax: {Dmax:.2f}')\n",
    "    ]\n",
    "    ax.legend(handles=handles, loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'N': N,\n",
    "        'Dmin': Dmin,\n",
    "        'D10': D10,\n",
    "        'D50': D50,\n",
    "        'D90': D90,\n",
    "        'Average': Dmean,\n",
    "        'Dmax': Dmax,\n",
    "        'diameters_cm': diam_cm\n",
    "    }\n",
    "\n",
    "# Example call:\n",
    "stats = draw_cdf(masks, pixel_size_mm=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fragment-segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
